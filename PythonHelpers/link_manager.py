#!/usr/bin/env python3
"""
Automated Link Manager for GitHub Pages
Manages LINK.txt file with all HTML files in the repository
"""

import os
import subprocess
import urllib.parse
from pathlib import Path


def get_repo_info():
    """Get repository owner and name from git remote"""
    try:
        # Get the remote URL
        result = subprocess.run(
            ['git', 'remote', 'get-url', 'origin'],
            capture_output=True,
            text=True,
            check=True
        )
        remote_url = result.stdout.strip()

        # Parse owner and repo name from various URL formats
        # Supports:
        # - git@github.com:owner/repo.git
        # - https://github.com/owner/repo.git
        # - http://local_proxy@127.0.0.1:PORT/git/owner/repo (local proxy format)

        # Remove .git suffix if present
        remote_url = remote_url.replace('.git', '')

        # Check for local proxy format first
        if '/git/' in remote_url:
            # Extract from: http://local_proxy@127.0.0.1:PORT/git/owner/repo
            parts = remote_url.split('/git/')[1].split('/')
            owner = parts[0]
            repo = parts[1] if len(parts) > 1 else parts[0]
            return owner, repo
        # Check for standard GitHub formats
        elif 'github.com' in remote_url:
            # Extract owner/repo from SSH format
            if remote_url.startswith('git@'):
                parts = remote_url.split(':')[1].split('/')
                owner = parts[0]
                repo = parts[1]
            # Extract from HTTPS format
            else:
                parts = remote_url.split('github.com/')[1].split('/')
                owner = parts[0]
                repo = parts[1]
            return owner, repo
        else:
            raise ValueError("Not a GitHub repository")

    except Exception as e:
        print(f"Error getting repository info: {e}")
        return None, None


def find_html_files(root_dir):
    """Find all HTML files in the repository"""
    html_files = []

    for root, dirs, files in os.walk(root_dir):
        # Skip .git directory
        if '.git' in dirs:
            dirs.remove('.git')

        for file in files:
            if file.endswith('.html'):
                # Get relative path from root
                rel_path = os.path.relpath(os.path.join(root, file), root_dir)
                html_files.append(rel_path)

    return sorted(html_files)


def generate_github_pages_url(owner, repo, file_path):
    """Generate GitHub Pages URL with proper encoding"""
    # URL encode the file path (handles spaces and special characters)
    encoded_path = urllib.parse.quote(file_path)
    return f"https://{owner}.github.io/{repo}/{encoded_path}"


def load_existing_descriptions(link_file):
    """Load existing descriptions from LINK.txt"""
    descriptions = {}

    if not os.path.exists(link_file):
        return descriptions

    try:
        with open(link_file, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                # Skip comments and empty lines
                if line.startswith('#') or not line:
                    continue

                # Parse URL and description
                if ' - ' in line:
                    url, description = line.split(' - ', 1)
                    descriptions[url.strip()] = description.strip()

    except Exception as e:
        print(f"Warning: Could not read existing LINK.txt: {e}")

    return descriptions


def update_link_file(owner, repo, html_files, link_file):
    """Update LINK.txt with all HTML files and their URLs"""

    # Load existing descriptions
    existing_descriptions = load_existing_descriptions(link_file)

    # Build the new content
    lines = [
        f"# GitHub Pages Links for {owner}/{repo}",
        "# Auto-generated by PythonHelpers/link_manager.py",
        ""
    ]

    for html_file in html_files:
        url = generate_github_pages_url(owner, repo, html_file)

        # Use existing description or create placeholder
        description = existing_descriptions.get(url, f"[Add description for {html_file}]")

        lines.append(f"{url} - {description}")

    # Add empty line at end
    lines.append("")

    # Write to file
    try:
        with open(link_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(lines))
        print(f"✓ Successfully updated {link_file}")
        print(f"  Found {len(html_files)} HTML file(s)")
    except Exception as e:
        print(f"✗ Error writing to {link_file}: {e}")


def main():
    """Main function"""
    # Get repository root (assume script is in PythonHelpers subdirectory)
    script_dir = Path(__file__).parent
    repo_root = script_dir.parent

    print("=" * 60)
    print("Automated Link Manager for GitHub Pages")
    print("=" * 60)

    # Get repository info
    print("\n1. Getting repository information...")
    owner, repo = get_repo_info()

    if not owner or not repo:
        print("✗ Could not determine repository owner/name")
        return 1

    print(f"   Repository: {owner}/{repo}")

    # Find HTML files
    print("\n2. Scanning for HTML files...")
    html_files = find_html_files(repo_root)

    if not html_files:
        print("✗ No HTML files found")
        return 1

    print(f"   Found {len(html_files)} HTML file(s):")
    for html_file in html_files:
        print(f"   - {html_file}")

    # Update LINK.txt
    print("\n3. Updating LINK.txt...")
    link_file = os.path.join(repo_root, 'LINK.txt')
    update_link_file(owner, repo, html_files, link_file)

    print("\n" + "=" * 60)
    print("✓ Link management complete!")
    print("=" * 60)
    print("\nNext steps:")
    print("1. Review LINK.txt and update any placeholder descriptions")
    print("2. Commit the updated LINK.txt with your changes")
    print("=" * 60)

    return 0


if __name__ == '__main__':
    exit(main())
